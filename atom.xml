<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-01-30T11:36:42.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>embedding在搜索和推荐中的应用(airbnb)</title>
    <link href="http://yoursite.com/2019/01/30/embedding%E5%9C%A8%E6%90%9C%E7%B4%A2%E5%92%8C%E6%8E%A8%E8%8D%90%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8-airbnb/"/>
    <id>http://yoursite.com/2019/01/30/embedding在搜索和推荐中的应用-airbnb/</id>
    <published>2019-01-30T11:36:14.000Z</published>
    <updated>2019-01-30T11:36:42.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><ul><li>大量的搜索历史数据的存储和分析使得搜索结果个性化成为可能</li><li>各平台优化目标不同：<br>1）新闻点击和停留时长<br>2）转化<br>3）买卖双方的收益</li><li>airbnb是为了让买卖双方都满意且成交；具体的：在房客给出地区和日期后，搜索结果中能够出现<strong>合适</strong>的房子</li></ul><h4 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h4><ul><li>实时个性化</li><li>负样本限定在同一市场中，增加同一市场内id的区分度</li><li>预定成功的作为global context</li><li>用类型代替ID，解决属性稀疏问题</li><li>Type embedding中 “房东拒绝”作为显式的负样本—&gt;减少将来失败的推荐<h4 id="主要工作"><a href="#主要工作" class="headerlink" title="主要工作"></a>主要工作</h4></li><li>800M的 search-click-session 构建listing embedding，作为短期兴趣个性化</li><li>50M的用户预订列表，训练用户type和listing type的embedding，来做长期兴趣个性化<h3 id="related-work"><a href="#related-work" class="headerlink" title="related work"></a>related work</h3><h3 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h3></li><li>listing embedding<ul><li>公式(负采样代替softmax)<br><img src="https://note.youdao.com/yws/public/resource/0fc79705c78fabbf85f6248b90e33549/xmlnote/CA1967BA35DC404396EE8E7D4468D136/21772"><br>Dp是正例，Dn是随机负例，Vb是预订的listing，Dmn 与正例在同一市场中的负例</li><li>1.选取的是search-click-session，间隔时间大于30min的算两个，停留时长&gt;30s,#click&gt;=2</li><li></li><li>正样本优化：预订的listing作为正样本”常驻”</li><li>负样本优化：采用同一市场中的listing，增加同一市场内的id区分度</li><li>冷启动，制定规则，取近邻的listing代替</li><li>评估：近邻review、聚类review</li></ul></li><li>user_type&amp;listing-type embedding<ul><li>使用用户预订的listing作为序列 </li><li>挑战：序列不够长，共性不稳定<ol><li>预定比点击小得多</li><li>预定数量很多是1</li><li>至少需要5-10个才能学到有意义的embedding</li><li>用户的喜好在改变，如：价格可能改变</li></ol></li><li>采用type这种方式，本质上是拆解成更一般的属性(解决数据稀疏、冷启动、用户属性变化的问题)<ul><li>平台、行业、广告主、尺寸、类型、ctr分段 </li><li>地域、年龄、性别、</li></ul></li><li>结构图：<br>  <img src="https://note.youdao.com/yws/public/resource/0fc79705c78fabbf85f6248b90e33549/xmlnote/A7AD3C265FD74EAEA499D57AE019C8D7/21808"></li><li>user_type和list_type在同一space训练</li><li>房主拒绝的作为负例，使embedding能够包含哪些user_type对哪些listing_type敏感，降低之后为新客推荐的拒绝率</li><li>公式：<br>  <img src="https://note.youdao.com/yws/public/resource/0fc79705c78fabbf85f6248b90e33549/xmlnote/A58EAAB23FAD4F0C9194F80D3F6DD018/21815"><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="如何训练"><a href="#如何训练" class="headerlink" title="如何训练"></a>如何训练</h4><h4 id="如何离线评估"><a href="#如何离线评估" class="headerlink" title="如何离线评估"></a>如何离线评估</h4><h4 id="在相似推荐中如何使用"><a href="#在相似推荐中如何使用" class="headerlink" title="在相似推荐中如何使用"></a>在相似推荐中如何使用</h4><h4 id="在搜索中如何使用"><a href="#在搜索中如何使用" class="headerlink" title="在搜索中如何使用"></a>在搜索中如何使用</h4></li></ul></li><li>搜索模型<x,y> GBDT训练 NDCG评估<ul><li>x表示特征，包含query、listing、user还有cross</li><li>y是label，包含view、click、book、reject</li></ul></x,y></li><li>embedding计算各种相似度，作为特征</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;introduction&quot;&gt;&lt;a href=&quot;#introduction&quot; class=&quot;headerlink&quot; title=&quot;introduction&quot;&gt;&lt;/a&gt;introduction&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;大量的搜索历史数据的存储和分析使得搜索结果个性化
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>CNN理解和可视化</title>
    <link href="http://yoursite.com/2019/01/26/CNN%E7%90%86%E8%A7%A3%E5%92%8C%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <id>http://yoursite.com/2019/01/26/CNN理解和可视化/</id>
    <published>2019-01-26T03:10:26.000Z</published>
    <updated>2019-02-26T03:10:48.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>目录</strong></p><ul><li>CNN网络结构</li><li>可视化方法<ul><li>卷积核可视化</li><li>最后一层 近邻/降维</li><li>激活点可视化</li><li>激活点反卷积</li><li>Saliency Maps</li><li>Class Model Visualisation/Gradient Ascent</li></ul></li><li>一个可视化工具</li><li>参考</li></ul><hr><h3 id="CNN网络结构"><a href="#CNN网络结构" class="headerlink" title="CNN网络结构"></a>CNN网络结构</h3><ul><li>Alexnet<br>  <img src="https://note.youdao.com/yws/public/resource/d7294646fdc5462e198f37173d1a8dda/xmlnote/FABD942364074377979F8CF2025652C2/21284" width="100%"><h3 id="可视化方法"><a href="#可视化方法" class="headerlink" title="可视化方法"></a>可视化方法</h3></li></ul><h5 id="卷积核可视化"><a href="#卷积核可视化" class="headerlink" title="卷积核可视化"></a>卷积核可视化</h5><ul><li><p>  <img src="https://note.youdao.com/yws/public/resource/d7294646fdc5462e198f37173d1a8dda/xmlnote/C7CBB393BDFC491882D189D878E49A88/21150" width="100%"></p></li><li><p>做法：将卷积核看做是一个图片，比如3X7X7的卷积核，可以将其看7X7的三通道图片</p></li><li>解释<ul><li>卷积核类似一个模板，与任意数据求内积，输出一个标量，在正则的约束下，那些与之模板类似的“局部图像”才会输出。(真.filter)</li><li>普遍性：使用不同的数据、第一层结果类似：有向边、角、相对的颜色等</li><li>这种方法仅适用于CNN的第一层，因为后面的卷积核不是直接对图片做操作，而是一些激活值，结果不好理解[<a href="http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture13.pdf" target="_blank" rel="external">第5页</a>]</li><li>从结果看，类似于传统CV的基础特征提取</li></ul></li><li>debug：well-trained networks usually display nice and smooth filters without any noisy patterns.Noisy patterns：训练时间不够，正则不够导致过拟合。[<a href="http://cs231n.github.io/understanding-cnn/" target="_blank" rel="external">图2上下文</a>][<a href="https://note.youdao.com/share/?id=eb4ba80d8cb9fe5d8c9ddbe833d1f3af&amp;type=notebook#/WEBa84c91672bc3205b17075726fd7d3f74" target="_blank" rel="external">论文</a>fig6]</li></ul><h5 id="最后一层-近邻-降维"><a href="#最后一层-近邻-降维" class="headerlink" title="最后一层 近邻/降维"></a>最后一层 近邻/降维</h5><ul><li><p>  <img src="https://note.youdao.com/yws/public/resource/d7294646fdc5462e198f37173d1a8dda/xmlnote/AE64886F40CB4388A6B8676A8BC0096B/21298" width="50%" height="50%"> </p></li><li><p>做法：图像-&gt;CNN网络-&gt;最后一层的向量-&gt;降维，将其<a href="https://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_6k.jpg" target="_blank" rel="external">可视化</a></p></li><li>解释：CNN最后的结果就是其对图片的特征提取，空间上相近就是图片相似(语义内容相似)</li></ul><h5 id="激活点可视化"><a href="#激活点可视化" class="headerlink" title="激活点可视化"></a>激活点可视化</h5><ul><li><p>实例</p><ul><li><p>分类任务的网络中有人脸特征的神经元</p><p>  <img src="https://note.youdao.com/yws/public/resource/eb4ba80d8cb9fe5d8c9ddbe833d1f3af/xmlnote/WEB24aeac63c2960ba94b3ef506932f20f6/74587362863A43C7866C93C3C2D08C38/21408" height="40%"></p></li><li><p>神经元寻找其学习的特征，输入图片的片段或全部，找出能让这个神经元激活值最大的topn，展示出来</p><p>  <img src="https://note.youdao.com/yws/public/resource/eb4ba80d8cb9fe5d8c9ddbe833d1f3af/xmlnote/WEB24aeac63c2960ba94b3ef506932f20f6/4ACA1E7253534960AC012700AFABCE89/21425"></p></li><li><p>Saliency vs Occlusion：遮挡部分区域后，看神经元激活值的变化/分类任务概率变化，了解特定神经元学到的是什么特征[<a href="http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture13.pdf" target="_blank" rel="external">第13页</a>]</p><p>  <img src="https://note.youdao.com/yws/public/resource/eb4ba80d8cb9fe5d8c9ddbe833d1f3af/xmlnote/WEB24aeac63c2960ba94b3ef506932f20f6/BB1EDFEBBFFB419A980CB30B9D4AE3DB/21428" width="70%"></p><h5 id="激活点反卷积"><a href="#激活点反卷积" class="headerlink" title="激活点反卷积"></a>激活点反卷积</h5></li></ul></li><li><p>  <img src="https://note.youdao.com/yws/public/resource/d7294646fdc5462e198f37173d1a8dda/xmlnote/02861F2792D44F658371AFE7EF9E6411/21303" width="70%"></p></li></ul><ul><li>原理：根据激活的值，反向找出它是被哪些图像(或片段)激活的，并且在原图上还原</li><li>还原的做法(反卷积<a href="https://note.youdao.com/share/?id=eb4ba80d8cb9fe5d8c9ddbe833d1f3af&amp;type=notebook#/WEBa84c91672bc3205b17075726fd7d3f74" target="_blank" rel="external">fig1</a>)：<ul><li>filter：卷积层可以与FC层相互<a href="https://blog.csdn.net/kekong0713/article/details/68941498" target="_blank" rel="external">转化</a>,使用卷积核的转置来卷积</li><li>Unpooling：记录了最大值得位置信息，然后将其他位置置0</li><li>Rectification</li></ul></li><li><p>底层是基础特征，与任务相关性弱；越高层，越综合，与任务相关性强[<a href="https://note.youdao.com/share/?id=eb4ba80d8cb9fe5d8c9ddbe833d1f3af&amp;type=notebook#/WEBa84c91672bc3205b17075726fd7d3f74" target="_blank" rel="external">fig2</a>]</p></li><li><p>底层容易学到，高层到后面才收敛[<a href="https://note.youdao.com/share/?id=eb4ba80d8cb9fe5d8c9ddbe833d1f3af&amp;type=notebook#/WEBa84c91672bc3205b17075726fd7d3f74" target="_blank" rel="external">fig4</a>]</p></li></ul><h5 id="Saliency-Maps"><a href="#Saliency-Maps" class="headerlink" title="Saliency Maps"></a>Saliency Maps</h5><p>*<br>    <img src="https://note.youdao.com/yws/public/resource/d7294646fdc5462e198f37173d1a8dda/xmlnote/923B29EE7694468BBC93DAF2A641B97D/21308" width="70%"></p><ul><li>目的：计算一个图片中各像素点对于分类结果的重要性/权重</li><li>原理：像素点加扰动，看神经元或分类如何变化）</li><li>做法：求一阶导数(输出关于各像素点的)</li></ul><h5 id="Class-Model-Visualisation-Gradient-Ascent"><a href="#Class-Model-Visualisation-Gradient-Ascent" class="headerlink" title="Class Model Visualisation/Gradient Ascent"></a>Class Model Visualisation/Gradient Ascent</h5><p>*<br>    <img src="https://note.youdao.com/yws/public/resource/d7294646fdc5462e198f37173d1a8dda/xmlnote/18ADF010FE6445299FBBBF46D029EEC2/21315" width="70%"></p><ul><li>目的：给定网络和激活点后，生成一张图片，描述该点捕获的特征</li><li>做法：求I<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">\arg\max_&#123;I&#125; S_c(I) - \lambda \|I\|_2^2</div></pre></td></tr></table></figure></li></ul><pre><code>1、 I是要生成的图片，用0或高斯噪声初始化repeat：2、前向计算目前图片的概率(在softmax之前)3、反向传播，获取神经元对于每个像素的梯度4、在原图上进行update</code></pre><ul><li>正则项的功能是使图像更自然，第二篇论文提出四种有效的正则方案。<h2 id="一个可视化工具"><a href="#一个可视化工具" class="headerlink" title="一个可视化工具"></a>一个可视化工具</h2>*<br>  <img src="https://note.youdao.com/yws/public/resource/eb4ba80d8cb9fe5d8c9ddbe833d1f3af/xmlnote/WEB24aeac63c2960ba94b3ef506932f20f6/27FA91184F714F4E8873FEB0FC46F2E7/21380" width="70%"></li><li><a href="https://github.com/yosinski/deep-visualization-toolbox" target="_blank" rel="external">https://github.com/yosinski/deep-visualization-toolbox</a></li></ul><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul><li>分享的主要框架参考这个：<a href="http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture13.pdf" target="_blank" rel="external">cs231n ppt关于理解和可视化的ppt</a> </li><li>相关论文<ul><li>Visualizing and Understanding Convolutional Networks</li><li>Understanding Neural Networks Through Deep Visualization<ul><li><a href="http://yosinski.com/deepvis" target="_blank" rel="external">code</a> 、<a href="https://blog.csdn.net/MargretWG/article/details/69786571" target="_blank" rel="external">相关博客</a></li></ul></li><li>Synthesizing the preferred inputs for neurons in neural networks via deep generator networks</li><li>Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks</li></ul></li><li>可视化工具视频：<a href="https://www.youtube.com/watch?v=AgkfIQ4IGaM" target="_blank" rel="external">https://www.youtube.com/watch?v=AgkfIQ4IGaM</a></li><li><a href="http://cs231n.stanford.edu/syllabus.html" target="_blank" rel="external">cs231n课程</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;目录&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CNN网络结构&lt;/li&gt;
&lt;li&gt;可视化方法&lt;ul&gt;
&lt;li&gt;卷积核可视化&lt;/li&gt;
&lt;li&gt;最后一层 近邻/降维&lt;/li&gt;
&lt;li&gt;激活点可视化&lt;/li&gt;
&lt;li&gt;激活点反卷积&lt;/li&gt;
&lt;li&gt;Salie
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>embedding在电商推荐中的应用(hema)</title>
    <link href="http://yoursite.com/2019/01/20/embedding%E5%9C%A8%E7%94%B5%E5%95%86%E6%8E%A8%E8%8D%90%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8-hema/"/>
    <id>http://yoursite.com/2019/01/20/embedding在电商推荐中的应用-hema/</id>
    <published>2019-01-20T11:52:46.000Z</published>
    <updated>2019-02-27T03:34:34.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ul><li>高维度带来的稀疏问题(sparsity problems due to the high dimension)</li><li>不能反映ID间的关系(cannot reflect the relationships among IDs, either homogeneous or heterogeneous ones)</li></ul><h2 id="相关背景"><a href="#相关背景" class="headerlink" title="相关背景"></a>相关背景</h2><ul><li><p>hema app</p><p>  <img src="https://note.youdao.com/yws/public/resource/f0fd42fa2d71dca057e320baf9933597/xmlnote/WEBRESOURCE84cecc3f356263c90a167951ffa56cbc/22292" width="70%" height="70%"><br>  <img src="../asset/blogImg/embedding_hema.jpeg" alt="hexo image"></p></li><li><p>各种id介绍</p><p>  <img src="https://note.youdao.com/yws/public/resource/f0fd42fa2d71dca057e320baf9933597/xmlnote/WEBRESOURCE868105f2419809155404e9764def13ad/22290" width="50%" height="50%">    </p></li></ul><h2 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h2><h3 id="符号说明"><a href="#符号说明" class="headerlink" title="符号说明"></a>符号说明</h3><pre><code>e：id要训练的向量C：滑动的上下文窗口N：用户数或语料的总量D：item的总量；类似于词典大小S：负样本的抽样集合IDs：一个商品的各种属性id集合M：从item向量映射到某一种属性向量的转移矩阵K：item所有属性的长度k：item的某一种属性，第一个属性是item自身的向量α、β：目标函数的权重因子V：一个属性下有多少个不同的商品w：V分之一，该商品占某种属性的比例item：具体商品user：用户attribute：商品的属性product：物品，如iPhoneXbrand：品牌store：商店cate：分类</code></pre><h3 id="item-id的模型"><a href="#item-id的模型" class="headerlink" title="item_id的模型"></a>item_id的模型</h3><ul><li><p>skip-grim框架</p><p>  <img src="https://note.youdao.com/yws/public/resource/f0fd42fa2d71dca057e320baf9933597/xmlnote/WEBRESOURCE8de80fae84f92a021a09ab3ce3faddb1/22293" width="70%" height="70%"> </p></li><li><p>skip-gram model损失函数</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">J=\frac&#123;1&#125;&#123;N&#125;\sum_&#123;n=1&#125;^N\sum_&#123;-C \leq j \leq C&#125;^&#123;1\leq n+j \leq N&apos;,j\neq0&#125;\log p(item_&#123;n+j&#125;|item_n))</div></pre></td></tr></table></figure></li><li><p>p的计算</p><ul><li><p>p通过softmax来获取</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">p(item_&#123;j&#125;|item_i)=\frac&#123;exp(e_j^&#123;&apos;T&#125;e_i)&#125;&#123;\sum_&#123;d=1&#125;^Dexp(e_d^&#123;&apos;T&#125;e_i)&#125;</div></pre></td></tr></table></figure></li><li><p>Mikolov et al. simplify NCE by <strong>negative-sampling</strong> [24] and replace <strong>softmax function</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">p(item_&#123;j&#125;|item_i)=\sigma(e_j^&#123;&apos;T&#125;e_i) \Pi_&#123;s=1&#125;^S\sigma(-e_s^&#123;&apos;T&#125;e_i)</div></pre></td></tr></table></figure></li><li><p>抽样方法的改进：negative samples are drawn from the <a href="https://zh.wikipedia.org/wiki/%E9%BD%8A%E5%A4%AB%E5%AE%9A%E5%BE%8B" target="_blank" rel="external">zipfian distribution</a></p></li></ul></li></ul><h3 id="属性id的模型"><a href="#属性id的模型" class="headerlink" title="属性id的模型"></a>属性id的模型</h3><ul><li><p>loss 函数</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">J=\frac&#123;1&#125;&#123;N&#125;(\sum_&#123;-C \le j \le C&#125;^&#123;1\le n+j \le N,j\ne 0&#125;logp(IDs(item_&#123;n+j&#125;)|IDs(item_n))+ \alpha logp(item_n|IDs(item_n))-\beta\sum_&#123;k=1&#125;^K||M_k||_2)</div></pre></td></tr></table></figure></li><li><p>第一部分:item的同现也意味着其属性id的同现</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">p(IDs(item_&#123;j&#125;)|IDs(item_i))=\sigma(\Sigma_&#123;k=1&#125;^K(w_&#123;jk&#125;e_&#123;jk&#125;^&#123;&apos;&#125;)^T(w_&#123;ik&#125;e_&#123;ik&#125;) )\Pi_&#123;s=1&#125;^S\sigma(-\Sigma_&#123;k=1&#125;^K(w_&#123;sk&#125;e_&#123;sk&#125;^&#123;&apos;&#125;)^T(w_&#123;ik&#125;e_&#123;ik&#125;))</div></pre></td></tr></table></figure></li></ul><ul><li><p>第二部分：对item_id和属性id的关系建模</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">p(item_n|IDs(item_n))=\sigma(\sum_&#123;k=2&#125;^Kw_&#123;ik&#125;e_&#123;i1&#125;^TM_ke_&#123;ik&#125;)</div><div class="line"></div><div class="line">    w_&#123;ik&#125;=\frac&#123;1&#125;&#123;V_&#123;ik&#125;&#125;</div><div class="line"></div><div class="line">    V_&#123;ik&#125;=\sum_&#123;j=1&#125;^DI(id_k(item_i)=id_k(item_j)) </div><div class="line"></div><div class="line">    I(x)=\begin&#123;cases&#125;0, x \ is \ False \\\\1, x \ is \ True \end&#123;cases&#125;</div></pre></td></tr></table></figure></li><li><p>第三部分：正则项</p></li></ul><h3 id="用户id的学习方法"><a href="#用户id的学习方法" class="headerlink" title="用户id的学习方法"></a>用户id的学习方法</h3><ul><li style="list-style: none"><input type="checkbox" checked> Average:updated frequently</li><li style="list-style: none"><input type="checkbox"> RNN</li></ul><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="item相似性推荐"><a href="#item相似性推荐" class="headerlink" title="item相似性推荐:"></a>item相似性推荐:</h3><ul><li>流程：user_id -&gt; items seed -&gt; sim items -&gt;Filtering -&gt; Ranking</li></ul><h3 id="item冷启动表示"><a href="#item冷启动表示" class="headerlink" title="item冷启动表示"></a>item冷启动表示</h3><ul><li>attribute ids–&gt;item id</li><li>given a new item, its corresponding product is likely sold elsewhere and the corresponding store has already sold other items.<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">e_&#123;i1&#125; \approx \sum_&#123;k=2&#125;^Kw_&#123;ik&#125;^Te_&#123;ik&#125;^TM_k^T</div></pre></td></tr></table></figure></li></ul><h3 id="平台之间用户迁移学习"><a href="#平台之间用户迁移学习" class="headerlink" title="平台之间用户迁移学习"></a>平台之间用户迁移学习</h3><p><img src="https://note.youdao.com/yws/public/resource/f0fd42fa2d71dca057e320baf9933597/xmlnote/WEBRESOURCE1746e21a9e8728895a230f6cc7d8436a/22288" width="70%" height="70%"> </p><ul><li>步骤<ul><li>根据item聚合计算source platform 的user_id</li><li>取交叉用户聚类</li><li>每一类中提取target domain的top item<ul><li>new coming user–&gt; items</li></ul></li><li>两种aggregating manners<ul><li>naive average</li><li>weighted average:含有目标平台中的item的才算<h3 id="学习其他任务"><a href="#学习其他任务" class="headerlink" title="学习其他任务"></a>学习其他任务</h3></li></ul></li></ul></li><li>store_id：sales forecast</li></ul><h2 id="借鉴与计划"><a href="#借鉴与计划" class="headerlink" title="借鉴与计划"></a>借鉴与计划</h2><ol><li>ad_id类比word，计算ad_id的embedding，得到user_id的广告特征</li><li>doc_id类比word，计算doc_id的embedding，得到user_id的内容特征</li><li>starSpace（关键在<strong>构造a,b+,b-</strong>;a和b的关系：<strong>标签、共现、因果、可跨域</strong>）</li></ol><pre><code>对象a | 正例b+ |负例b- |备注---|---|---|---ad_id |被同一用户点击过的ad_id|随机|共现doc_id | 被同一用户点击过的doc_id|随机|共现创意图片提特征| 行业作为标签 |随机|标签...|...|...|...user_id | doc_id |未点击过的doc|标签图片特征 | 同现的图片特征某一特征 |随机|属性共现</code></pre><ol><li>embedding在ctr模型和cvr模型中的应用</li><li>广告创意图片特征的提取（计划中）</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题&quot;&gt;&lt;a href=&quot;#问题&quot; class=&quot;headerlink&quot; title=&quot;问题&quot;&gt;&lt;/a&gt;问题&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;高维度带来的稀疏问题(sparsity problems due to the high dimension)&lt;/li&gt;
&lt;li
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>CNN基础</title>
    <link href="http://yoursite.com/2018/08/26/CNN%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2018/08/26/CNN基础/</id>
    <published>2018-08-26T03:14:11.000Z</published>
    <updated>2019-02-26T03:14:42.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">http://cs231n.github.io/convolutional-networks/</a></li></ul><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><ul><li>与常规NN比：减少的了参数的规模，filter是和特定、局部的输入连接<h4 id="主要构成"><a href="#主要构成" class="headerlink" title="主要构成"></a>主要构成</h4></li><li>input layer (W1 <em> H1 </em> D1)</li><li>conv layer<ul><li>主要参数<ul><li>K: depth(卷积核的数量，决定输出的层数或者深度) </li><li>S: stride</li><li>P: zero-padding:长宽对齐、卷积时补齐</li></ul></li><li>卷积核权重共享(Parameter Sharing)<ul><li>一次卷积中共享核的参数，是基于：当特征在某一个区域重要时，它在其他地方也会比较重要；少数反例：中间构图的图片</li><li>可以极大地降低参数数量</li></ul></li><li>输出规模：<ul><li>W2=(W1-F+2P)/S+1  </li><li>H2=(H1-F+2P)/S+1              </li><li>D2=K :卷积核的个数</li></ul></li><li>矩阵乘法的实现：im2col 矩阵展开</li><li>1<code>*</code>1 卷积核也是是有效的，因为它暗含一个维度，如图片中是1<em>1</em>3  </li><li>Dilated convolutions：w[0]<em>x[0] + w[1]</em>x[2] + w[2]*x[4] 可以扩大卷积的视野，在图像分割中使用较多</li></ul></li><li>relu：无参数 neg–&gt;0</li><li>pool：无训练参数<ul><li>降低参数规模、防止过拟合</li><li>两种常见的pooling layer：<ul><li>F=2 S=2</li><li>F=3 S=2</li></ul></li><li>干掉pooling层<ul><li>可以通过变大conv层的步幅来降低参数数量</li><li>生成模型如VAEs何GANs去掉pooling层效果不过，可能是趋势</li></ul></li></ul></li><li>fc层：FC层可以和卷积层相互转换<h4 id="卷积网络架构"><a href="#卷积网络架构" class="headerlink" title="卷积网络架构"></a>卷积网络架构</h4><h5 id="常见架构模式"><a href="#常见架构模式" class="headerlink" title="常见架构模式"></a>常见架构模式</h5><ul><li>INPUT -&gt; [[CONV -&gt; RELU]<em>N -&gt; POOL?]</em>M -&gt; [FC -&gt; RELU]*K -&gt; FC<ul><li>多个小filter VS 一个大filter</li><li>前者由于中间有多个非线性激活函数，所以表达能力更强；且参数数量更小</li><li>实际中的劣势是，训练时需要保存的层数多，占用空间大</li><li>实践：不逞英雄，在现有的ImageNet上finetune(微调)<h5 id="各层参数量级大小选择"><a href="#各层参数量级大小选择" class="headerlink" title="各层参数量级大小选择"></a>各层参数量级大小选择</h5></li><li>输入层：一般为2的次幂：32、64、224、384、512</li><li>卷积层：小filter</li><li>池化层：field：2*2 stride=2</li><li>stride=1 让conv专注提取特征，pooling层专注采样</li><li>padding的意义在于保持输出的尺寸一致，另外也能防止边缘信息不被“冲洗”，所以也能提升性能</li><li>内存限制妥协：第一层使用大卷积核，可以降低卷积层的输出量</li></ul></li></ul></li></ul><h4 id="new"><a href="#new" class="headerlink" title="new"></a>new</h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://cs231n.github.io/convolutional-networks/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://cs231n.github.io/convolutional-netw
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>非监督模型评估</title>
    <link href="http://yoursite.com/2018/03/26/%E9%9D%9E%E7%9B%91%E7%9D%A3%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/"/>
    <id>http://yoursite.com/2018/03/26/非监督模型评估/</id>
    <published>2018-03-26T03:18:55.000Z</published>
    <updated>2019-02-26T03:20:55.000Z</updated>
    
    <content type="html"><![CDATA[<p># </p><ul><li><p>lda评估：</p><ul><li><p>intrinsic：不借助其他手段，混乱度</p><ul><li>混乱度：(perplexity)  ：语言模型的评估标准，直观理解是预测下一个词时有多少种选择，和熵的意义相似，越小代表信息量越小，越不混乱，越好。</li><li>为什么在语言模型中有效：给定测试数据都是合法的，相当于正样本，比较几个模型，计算出来的混乱度越小，越接近正样本，越好</li><li>计算公式<br><img src="DBC98653-043B-44E8-9DB7-B663E872DEE4.png" alt=""></li><li>Lda作者在选定topic数时，使用了混乱度，在评估不同模型时不太适合。另外这种方案与人工评估有出入。</li><li>给定文档中，根据doc-topic和topic-word矩阵计算各词的出现概率</li><li>参考：<ul><li><a href="http://qpleple.com/perplexity-to-evaluate-topic-models/" target="_blank" rel="external">Perplexity To Evaluate Topic Models</a></li><li><a href="https://www.cnblogs.com/nlp-yekai/p/3816532.html" target="_blank" rel="external">用python计算lda语言模型的困惑度并作图 - 叶落花开 - 博客园</a></li><li><a href="http://blog.csdn.net/dongweionly/article/details/50286961" target="_blank" rel="external">LDA perplexity计算 - CSDN博客</a></li></ul></li></ul></li><li><p>可解释性</p><ul><li>词的主题向量，降维，聚类、可视化，词的语义表示分布</li><li>主题的词分布</li></ul></li><li>在下游任务中评估<ul><li>句子语义表示：分词、预处理、预测</li><li>语义匹配：</li><li>词的表示：训练完成后即可拿到，字典</li></ul></li></ul></li></ul><p>#nlp/lda#</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;# &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;lda评估：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;intrinsic：不借助其他手段，混乱度&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;混乱度：(perplexity)  ：语言模型的评估标准，直观理解是预测下一个词时有多少种选择，和熵的意义相似，越小代表
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>统计分布总结</title>
    <link href="http://yoursite.com/2018/02/26/%E7%BB%9F%E8%AE%A1%E5%88%86%E5%B8%83%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2018/02/26/统计分布总结/</id>
    <published>2018-02-26T03:16:19.000Z</published>
    <updated>2019-02-26T03:16:37.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="分布"><a href="#分布" class="headerlink" title="分布"></a>分布</h1><p>#statistics/distribution#</p><ul><li>伯努利试验是最基本的过程：只有<strong>两种</strong>可能的<strong>单次</strong>随机试验<ol><li>两种可能，即0和1，所以其分布又叫两点分布或0-1分布</li><li>单次，只能进行一次，次数多了就构成二项分布，这是后话。</li></ol></li><li>伯努利分布<ul><li>伯努利分布是成功和失败的这两个点的分布，是离散的</li></ul></li><li><p>二项分布</p><ul><li>n重伯努利试验后，成功的次数n的分布</li><li>与伯努利分布的主要不同<pre><code>1. 试验的次数变多了，1次———&gt;多次2. 自变量x的取值不一样了，成功／失败的概率————&gt;成功(正面)次数k的概率  </code></pre></li><li>自变量：一维离散，参数：实验次数和一个定值概率<img src="&amp;&amp;&amp;SFLOCALFILEPATH&amp;&amp;&amp;D4D2591A-FD8B-4DFB-9B92-4A66EB110D5C.png" alt=""></li></ul></li><li><p>多项分布</p><ul><li>伯努利事件中的两种可能变成多种可能，升级的二项分布</li><li>与二项分布的主要不同：硬币变成骰子，自变量x取值增加维度：比如抛6次骰子，问6朝上3次，1朝上2次，2朝上1次的概率就服从多项分布，如果问6朝上3次，其他朝上3次的概率就成了二项分布</li><li>自变量是2维，比如投骰子，k=6，自变量为(1,1,0,0,8,3)就表示抛的结果是1次1，1次2，8次5，3次6的概率</li><li>参数是n和p1，p2……Pk，抛骰子的实验来说的话，n表示实验做了n次，p表示抛出每个面的概率，所有6个p相加为1</li><li>应变量就是出现自变量这种结果的概率</li><li>概率密度函数的两种写法，最后提到的共轭分布是狄利克雷分布<img src="&amp;&amp;&amp;SFLOCALFILEPATH&amp;&amp;&amp;7A2601D2-10CE-41B6-A2EA-0316B928A314.png" alt=""></li><li></li><li>与其他分布的关系：<ul><li>n=1，k=2:伯努利分布</li><li>n&gt;1，k=2:二项分布</li><li>n=1 广义伯努利分布或分类分布</li></ul></li></ul></li><li><p>beta分布：</p><ul><li>分布上的分布，基于先验的分布，根据目前的样本调整得到的分布。</li><li>beta分布的实验和二项分布一样，都是n多次二值实验，但用途有很大不同，二项分布是已经了解（或假定已经了解）事件发生的概率，去求多次实验时结果的分布，而beta分布最见的用途是：我们不了解事件发生的概率，但又一些合理的猜测（<strong>先验</strong>），可以将其加入到最终的分布中（贝叶斯学派）</li><li>beta分布是在二项分布上的分布，之前的二项分布和多项式分布中，我们假定随机试验的概率θ是已知的常数，比如，硬币正面的概率就是0.5，如果不均匀那也是一个不是0.5的定值，而骰子抛出2的概率我们也假定就是1/6，但是在beta分布中，θ是个分布(它由两个参数决定)</li><li>考虑先验有意义，举例：<a href="https://www.zhihu.com/question/30269898" target="_blank" rel="external">https://www.zhihu.com/question/30269898</a><ul><li>我们想预测一个棒球运动员的棒球击中率，可以使用二项分布来做，比如让他打几个球，算一下他的击中率，然后就可以预测他能击中几个球的概率了。用二项分布计算的重点是多个球时的击中概率，这个没错，问题是我们让运动员打了几个球，用这几个球的结果计算出来的概率可信度有多大呢。不大的原因是比如一般运动员的击中概率是0.27，但实验中打了5个球，心情好，全中，你敢说这个运动员的击中率是100%吗，全不中，敢说是0%吗，不敢啊，是因为不符合常理的0.27（先验），用上这个0.27这个数据得出来的数据就比较可信了</li></ul></li><li>概率密度函数  <ul><li>x是一个概率，y即f(x)是概率是x的概率，参数代表先验信息<img src="&amp;&amp;&amp;SFLOCALFILEPATH&amp;&amp;&amp;F0C4ADAC-E980-4ED1-9BBE-8AADAFC32CD4.png.png" alt=""></li></ul></li></ul></li><li><p>Dirichlet Distribution</p><ul><li>理解可以比对beta分布，只要将二值结果变为多值就可以了   </li><li><p>概率密度函数</p><ul><li>x1……xK为自变量，物理意义是每个类别的概率，和为1，是个分布</li><li>alpha为参数，包含先验信息<img src="&amp;&amp;&amp;SFLOCALFILEPATH&amp;&amp;&amp;0E2E561D-2ADB-427C-BFDE-79A1B209B8E0.png" alt=""></li></ul></li><li><p>可视化理解参数alpha变化：<a href="https://en.wikipedia.org/wiki/Dirichlet_distribution#/media/File:LogDirichletDensity-alpha_0.3_to_alpha_2.0.gif" target="_blank" rel="external">Dirichlet distribution - Wikipedia</a></p></li></ul></li><li>狄利克雷过程<br>分布的分布？隐含的变量</li></ul><ul><li>最大似然函数和最大后验估计<ul><li>最大似然函数是：给定样本带入给定的似然函数中，参数作为自变量，函数最大时对应的值，可以理解为：最拟合目前的样本的参数，是个值</li><li>最大后验分布<ul><li>求那个参数时，考虑先验</li><li>这个参数，不是一个值，而是服从一定的分布，往往是beta分布或dirichlet分布，因为是分布，预测时，要把分布中所有可能的参数都算一下，求和，连续的求积分</li><li>同样，也是求最大，后验分布最大时的那个值</li></ul></li></ul></li></ul><ul><li>参考<ul><li><a href="https://en.wikipedia.org/wiki/Beta_distribution" target="_blank" rel="external">Beta distribution - Wikipedia</a></li><li><a href="https://en.wikipedia.org/wiki/Multinomial_distribution" target="_blank" rel="external">Multinomial distribution - Wikipedia</a></li><li><a href="https://en.wikipedia.org/wiki/Dirichlet_distribution" target="_blank" rel="external">Dirichlet distribution - Wikipedia</a></li><li><a href="https://www.zhihu.com/question/30269898" target="_blank" rel="external">https://www.zhihu.com/question/30269898</a></li><li></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;分布&quot;&gt;&lt;a href=&quot;#分布&quot; class=&quot;headerlink&quot; title=&quot;分布&quot;&gt;&lt;/a&gt;分布&lt;/h1&gt;&lt;p&gt;#statistics/distribution#&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;伯努利试验是最基本的过程：只有&lt;strong&gt;两种&lt;/stro
      
    
    </summary>
    
    
  </entry>
  
</feed>
