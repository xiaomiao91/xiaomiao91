<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> CNN理解和可视化 · Hexo</title><meta name="description" content="CNN理解和可视化 - John Doe"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://yoursite.com/atom.xml" title="Hexo"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://weibo.com/u/2266458253" target="_blank" class="nav-list-link">WEIBO</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">CNN理解和可视化</h1><div class="post-info">Jan 26, 2019</div><div class="post-content"><p><strong>目录</strong></p>
<ul>
<li>CNN网络结构</li>
<li>可视化方法<ul>
<li>卷积核可视化</li>
<li>最后一层 近邻/降维</li>
<li>激活点可视化</li>
<li>激活点反卷积</li>
<li>Saliency Maps</li>
<li>Class Model Visualisation/Gradient Ascent</li>
</ul>
</li>
<li>一个可视化工具</li>
<li>参考</li>
</ul>
<hr>
<h1 id="CNN网络结构"><a href="#CNN网络结构" class="headerlink" title="CNN网络结构"></a>CNN网络结构</h1><ul>
<li>Alexnet  <img src="/2019/01/26/CNN理解和可视化/what_cnn_learn.png" alt="what_cnn_learn.png" title="what_cnn_learn.png">
</li>
</ul>
<h1 id="可视化方法"><a href="#可视化方法" class="headerlink" title="可视化方法"></a>可视化方法</h1><h2 id="卷积核可视化"><a href="#卷积核可视化" class="headerlink" title="卷积核可视化"></a>卷积核可视化</h2><ul>
<li>  <img src="/2019/01/26/CNN理解和可视化/first_layer.png" alt="first_layer.png" title="first_layer.png"></li>
<li>做法：将卷积核看做是一个图片，比如3X7X7的卷积核，可以将其看7X7的三通道图片</li>
<li>解释<ul>
<li>卷积核类似一个模板，与任意数据求内积，输出一个标量，在正则的约束下，那些与之模板类似的“局部图像”才会输出。(真.filter)</li>
<li>普遍性：使用不同的数据、第一层结果类似：有向边、角、相对的颜色等</li>
<li>这种方法仅适用于CNN的第一层，因为后面的卷积核不是直接对图片做操作，而是一些激活值，结果不好理解[<a href="http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture13.pdf" target="_blank" rel="external">第5页</a>]</li>
<li>从结果看，类似于传统CV的基础特征提取</li>
</ul>
</li>
<li>debug：well-trained networks usually display nice and smooth filters without any noisy patterns.Noisy patterns：训练时间不够，正则不够导致过拟合。[<a href="http://cs231n.github.io/understanding-cnn/" target="_blank" rel="external">图2上下文</a>][<a href="https://note.youdao.com/share/?id=eb4ba80d8cb9fe5d8c9ddbe833d1f3af&amp;type=notebook#/WEBa84c91672bc3205b17075726fd7d3f74" target="_blank" rel="external">论文</a>fig6]</li>
</ul>
<h2 id="最后一层-近邻-降维"><a href="#最后一层-近邻-降维" class="headerlink" title="最后一层 近邻/降维"></a>最后一层 近邻/降维</h2><ul>
<li>  <img src="/2019/01/26/CNN理解和可视化/classify_visualization.png" alt="classify_visualization.png" title="classify_visualization.png">
</li>
</ul>
<ul>
<li>做法：图像-&gt;CNN网络-&gt;最后一层的向量-&gt;降维，将其<a href="https://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_6k.jpg" target="_blank" rel="external">可视化</a></li>
<li>解释：CNN最后的结果就是其对图片的特征提取，空间上相近就是图片相似(语义内容相似)</li>
</ul>
<h2 id="激活点可视化"><a href="#激活点可视化" class="headerlink" title="激活点可视化"></a>激活点可视化</h2><ul>
<li><p>实例</p>
<ul>
<li><p>分类任务的网络中有人脸特征的神经元</p>
  <img src="/2019/01/26/CNN理解和可视化/face_detect.png" alt="face_detect.png" title="face_detect.png">
</li>
<li><p>神经元寻找其学习的特征，输入图片的片段或全部，找出能让这个神经元激活值最大的topn，展示出来</p>
  <img src="/2019/01/26/CNN理解和可视化/edge.png" alt="edge.png" title="edge.png">
</li>
<li><p>Saliency vs Occlusion：遮挡部分区域后，看神经元激活值的变化/分类任务概率变化，了解特定神经元学到的是什么特征[<a href="http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture13.pdf" target="_blank" rel="external">第13页</a>]</p>
  <img src="/2019/01/26/CNN理解和可视化/which_matters.png" alt="which_matters.png" title="which_matters.png">
</li>
</ul>
</li>
</ul>
<h2 id="激活点反卷积"><a href="#激活点反卷积" class="headerlink" title="激活点反卷积"></a>激活点反卷积</h2><ul>
<li>  <img src="/2019/01/26/CNN理解和可视化/layer2.png" alt="layer2.png" title="layer2.png">
</li>
<li><p>原理：根据激活的值，反向找出它是被哪些图像(或片段)激活的，并且在原图上还原</p>
</li>
<li>还原的做法(反卷积<a href="https://note.youdao.com/share/?id=eb4ba80d8cb9fe5d8c9ddbe833d1f3af&amp;type=notebook#/WEBa84c91672bc3205b17075726fd7d3f74" target="_blank" rel="external">fig1</a>)：<ul>
<li>filter：卷积层可以与FC层相互<a href="https://blog.csdn.net/kekong0713/article/details/68941498" target="_blank" rel="external">转化</a>,使用卷积核的转置来卷积</li>
<li>Unpooling：记录了最大值得位置信息，然后将其他位置置0</li>
<li>Rectification</li>
</ul>
</li>
<li><p>底层是基础特征，与任务相关性弱；越高层，越综合，与任务相关性强[<a href="https://note.youdao.com/share/?id=eb4ba80d8cb9fe5d8c9ddbe833d1f3af&amp;type=notebook#/WEBa84c91672bc3205b17075726fd7d3f74" target="_blank" rel="external">fig2</a>]</p>
</li>
<li><p>底层容易学到，高层到后面才收敛[<a href="https://note.youdao.com/share/?id=eb4ba80d8cb9fe5d8c9ddbe833d1f3af&amp;type=notebook#/WEBa84c91672bc3205b17075726fd7d3f74" target="_blank" rel="external">fig4</a>]</p>
</li>
</ul>
<h2 id="Saliency-Maps"><a href="#Saliency-Maps" class="headerlink" title="Saliency Maps"></a>Saliency Maps</h2><p>*<br>    <img src="/2019/01/26/CNN理解和可视化/ad_pic_important.png" alt="ad_pic_important.png" title="ad_pic_important.png"></p>
<ul>
<li>目的：计算一个图片中各像素点对于分类结果的重要性/权重</li>
<li>原理：像素点加扰动，看神经元或分类如何变化）</li>
<li>做法：求一阶导数(输出关于各像素点的)</li>
</ul>
<h2 id="Class-Model-Visualisation-Gradient-Ascent"><a href="#Class-Model-Visualisation-Gradient-Ascent" class="headerlink" title="Class Model Visualisation/Gradient Ascent"></a>Class Model Visualisation/Gradient Ascent</h2><p>*<br>    <img src="/2019/01/26/CNN理解和可视化/unconv.png" alt="unconv.png" title="unconv.png"></p>
<ul>
<li>目的：给定网络和激活点后，生成一张图片，描述该点捕获的特征</li>
<li>做法：求I<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">\arg\max_&#123;I&#125; S_c(I) - \lambda \|I\|_2^2</div></pre></td></tr></table></figure>
</li>
</ul>
<pre><code>1、 I是要生成的图片，用0或高斯噪声初始化
repeat：
2、前向计算目前图片的概率(在softmax之前)
3、反向传播，获取神经元对于每个像素的梯度
4、在原图上进行update
</code></pre><ul>
<li>正则项的功能是使图像更自然，第二篇论文提出四种有效的正则方案。</li>
</ul>
<h1 id="一个可视化工具"><a href="#一个可视化工具" class="headerlink" title="一个可视化工具"></a>一个可视化工具</h1><p>*<br>    <img src="/2019/01/26/CNN理解和可视化/cnn_tool.png" alt="cnn_tool.png" title="cnn_tool.png"></p>
<ul>
<li><a href="https://github.com/yosinski/deep-visualization-toolbox" target="_blank" rel="external">https://github.com/yosinski/deep-visualization-toolbox</a></li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li>分享的主要框架参考这个：<a href="http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture13.pdf" target="_blank" rel="external">cs231n ppt关于理解和可视化的ppt</a> </li>
<li>相关论文<ul>
<li>Visualizing and Understanding Convolutional Networks</li>
<li>Understanding Neural Networks Through Deep Visualization<ul>
<li><a href="http://yosinski.com/deepvis" target="_blank" rel="external">code</a> 、<a href="https://blog.csdn.net/MargretWG/article/details/69786571" target="_blank" rel="external">相关博客</a></li>
</ul>
</li>
<li>Synthesizing the preferred inputs for neurons in neural networks via deep generator networks</li>
<li>Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks</li>
</ul>
</li>
<li>可视化工具视频：<a href="https://www.youtube.com/watch?v=AgkfIQ4IGaM" target="_blank" rel="external">https://www.youtube.com/watch?v=AgkfIQ4IGaM</a></li>
<li><a href="http://cs231n.stanford.edu/syllabus.html" target="_blank" rel="external">cs231n课程</a></li>
</ul>
</div></article></div></main><footer><div class="paginator"><a href="/2019/01/30/embedding在搜索和推荐中的应用-airbnb/" class="prev">PREV</a><a href="/2019/01/20/embedding在电商推荐中的应用-hema/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2019 <a href="http://yoursite.com">John Doe</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script></body></html>